run:
  task: 'DatasetPretrainedModelTrainTask'
  runner: 'BaseTrainer'
  seed: 2024
  max_seq_length: 64


model:
  model_cls: 'CLIPModel'
  config:
    pretrained_model_name_or_path: &model_name_or_path 'openai/clip-vit-large-patch14-336'
  lora:
    r: 16
    auto_mapping: null
    base_model_name_or_path: *model_name_or_path
    bias: 'none'
    fan_in_fan_out: False
    init_lora_weights: True
    inference_mode: False
    layers_pattern: null
    layers_to_transform: null
    lora_alpha: 32
    lora_dropout: 0.1
    revision: null
    task_type: 'FEATURE_EXTRACTION'
    target_modules: 'all-linear'


processor:
  processor_cls: "CLIPProcessor"
  config:
    pretrained_model_name_or_path: 'openai/clip-vit-large-patch14-336'

dataset:
  'COCOCaptionsWithNegCLIPHNDatasetBuilder':
    split:
      - 'train'
      - 'restval'

trainer:
  output_dir: '/mnt/working/multimodal/_output'
  learning_rate: &learning_rate 1.0e-5
  lr_scheduler_type: 'cosine'
  warmup_steps: &warmup_steps 50
  weight_decay: &weight_decay 1.0e-1
  max_steps : &max_steps 500
  save_steps: 50
  logging_steps : 1
  evaluation_strategy: 'no' # 'steps', 'epochs', 'no'

  num_train_epochs: 1
  per_device_train_batch_size : &per_device_batch_size 1024
  gradient_accumulation_steps: &gradient_accumulation 1
  # For memory efficiency.
  gradient_checkpointing : True
  # NOTE : need to use this option to cope with DDP
  gradient_checkpointing_kwargs:
    use_reentrant: False
  ddp_find_unused_parameters: False

  fp16: &fp16 True # use torch.float16
  fp16_opt_level : '01'
  bf16: &bf16 False # Use torch.bfloat16

  group_by_length : False # Whether to order the sample by token length.
  use_cpu : False
  remove_unused_columns : False