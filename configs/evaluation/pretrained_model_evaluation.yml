run:
  task: 'PretrainedModelEvaluateTask'
  seed: 2024
  output_dir: &output_dir '/mnt/working/_result/clip_vit_base'

model:
  model_cls: 'CLIPModel'
  config:
    pretrained_model_name_or_path: 'openai/clip-vit-base-patch32'

processor:
  processor_cls: "CLIPProcessor"
  config:
    pretrained_model_name_or_path: 'openai/clip-vit-base-patch32'

dataset:
  - Flickr30kDatasetBuilder:
      split: 'test'
  - COCOCaptionsDatasetBuilder:
      split: 'test'
  - ARODatasetBuilder:
  - CREPEDatasetBuilder:
  - SUGARCREPEDatasetBuilder:
  - SVODatasetBuilder:
  - WinogroundDatasetBuilder:

evaluator:
  - RetrievalEvaluator:
      collator:
        collator_cls: 'ImageCollator'
        config:
          max_seq_length: &max_seq_length 64
      k_values:
        - 1
        - 5
        - 10
      output_dir: *output_dir
  - RetrievalEvaluator:
      collator:
        collator_cls: 'ImageURLCollator'
        config:
          max_seq_length: *max_seq_length
      k_values:
        - 1
        - 5
        - 10
      output_dir: *output_dir
  - AROEvaluator:
      collator:
        collator_cls: 'ImageCollator'
        config:
          max_seq_length: *max_seq_length
      output_dir: *output_dir
  - CrepeEvaluator:
      collator:
        collator_cls: 'ImageCollator'
        config:
          max_seq_length: *max_seq_length
      output_dir: *output_dir
  - SugarCrepeEvaluator:
      collator:
        collator_cls: 'ImageCollator'
        config:
          max_seq_length: *max_seq_length
      output_dir: *output_dir
  - SVOEvaluator:
      collator:
        collator_cls: 'ImageCollator'
        config:
          max_seq_length: *max_seq_length
      output_dir: *output_dir
  - WinogroundEvaluator:
      collator:
        collator_cls: 'ImageCollator'
        config:
          max_seq_length: *max_seq_length
      output_dir: *output_dir
